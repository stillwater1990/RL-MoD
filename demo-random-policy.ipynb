{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "\n",
    "from env import Scenario, AMoD, CascadedQLearning\n",
    "from util import mat2str, dictsum, moving_average\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1 <br>\n",
    "\n",
    "Given a 2x4 grid:\n",
    "\n",
    "| 0 | 2 | 4 | 6 |<br>\n",
    "| 1 | 3 | 5 | 7 | <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume the demand is generated according to K=2 periodic patterns. That is:\n",
    "\n",
    "\n",
    "- K = 1 --> people go from 0 to 7 and from 6 to 1;\n",
    "- K = 2 --> people go from 7 to 0 and from 1 to 6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy agent\n",
    "agent = CascadedQLearning(env=env)\n",
    "num_nodes = len(agent.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 100 | Reward: 173.16 | Revenue: 0.00 | ServedDemand: 80.00 | Reb. Cost: 186.00: 100%|██████████| 100/100 [03:38<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test Episodes\n",
    "test_episodes = 100\n",
    "epochs = trange(test_episodes) # build tqdm iterator for loop visualization\n",
    "max_steps = 100 # maximum length of episode\n",
    "np.random.seed(10)\n",
    "\n",
    "# book-keeping variables\n",
    "test_rewards = []\n",
    "test_revenue = []\n",
    "test_served_demand = []\n",
    "test_rebalancing_cost = []\n",
    "\n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            # Execure Random Policy\n",
    "            action_rl = [] # action for all nodes\n",
    "            for i in range(num_nodes):\n",
    "                action_rl.append(np.random.randint(low=0, high=4))\n",
    "            # get actual vehicle distributions vi (i.e. (x1*x2*..*xn)*num_vehicles)\n",
    "            v_d = agent.get_desired_distribution(action_rl)\n",
    "\n",
    "            # 1.3 Solve ILP - Minimal Distance Problem \n",
    "            t = env.time\n",
    "            accTuple = [(n,env.acc[n][t]) for n in env.acc]\n",
    "            accRLTuple = [(n, int(v_d_n)) for n, v_d_n in enumerate(v_d)]\n",
    "            edgeAttr = [(i,j,env.G.edges[i,j]['time']) for i,j in env.G.edges]\n",
    "            modPath = os.getcwd().replace('\\\\','/')+'/mod/'\n",
    "            OPTPath = os.getcwd().replace('\\\\','/')+'/OPT/Random/S1/'\n",
    "            if not os.path.exists(OPTPath):\n",
    "                os.makedirs(OPTPath)\n",
    "            datafile = OPTPath + f'data_{t}.dat'\n",
    "            resfile = OPTPath + f'res_{t}.dat'\n",
    "            with open(datafile,'w') as file:\n",
    "                file.write('path=\"'+resfile+'\";\\r\\n')\n",
    "                file.write('edgeAttr='+mat2str(edgeAttr)+';\\r\\n')\n",
    "                file.write('accInitTuple='+mat2str(accTuple)+';\\r\\n')\n",
    "                file.write('accRLTuple='+mat2str(accRLTuple)+';\\r\\n')\n",
    "            modfile = modPath+'minRebDistRebOnly.mod'\n",
    "            CPLEXPATH = \"C:/Program Files/ibm/ILOG/CPLEX_Studio1210/opl/bin/x64_win64/\"\n",
    "            my_env = os.environ.copy()\n",
    "            my_env[\"LD_LIBRARY_PATH\"] = CPLEXPATH\n",
    "            out_file =  OPTPath + f'out_{t}.dat'\n",
    "            with open(out_file,'w') as output_f:\n",
    "                subprocess.check_call([CPLEXPATH+\"oplrun\", modfile, datafile], stdout=output_f, env=my_env)\n",
    "            output_f.close()\n",
    "\n",
    "            # 3. collect results from file\n",
    "            flow = defaultdict(float)\n",
    "            with open(resfile,'r', encoding=\"utf8\") as file:\n",
    "                for row in file:\n",
    "                    item = row.strip().strip(';').split('=')\n",
    "                    if item[0] == 'flow':\n",
    "                        values = item[1].strip(')]').strip('[(').split(')(')\n",
    "                        for v in values:\n",
    "                            if len(v) == 0:\n",
    "                                continue\n",
    "                            i,j,f = v.split(',')\n",
    "                            flow[int(i),int(j)] = float(f)\n",
    "            action = [flow[i,j] for i,j in env.edges]\n",
    "\n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward # update sum of rewards over episode\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            obs = new_obs\n",
    "\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_rebalancing_cost:.2f}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_revenue.append(episode_revenue)\n",
    "        test_served_demand.append(episode_served_demand)\n",
    "        test_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Performance: \n",
      "\n",
      "Avg Reward: 133.96\n",
      "Total Revenue: 0.00\n",
      "Total Served Demand: 68.89\n",
      "Total Rebalancing Cost: 190.98\n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(test_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(test_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(test_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(test_rebalancing_cost):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2x4 grid:\n",
    "\n",
    "| 0 | 2 | 4 | 6 |<br>\n",
    "| 1 | 3 | 5 | 7 | <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume the demand is generated according to an unbalanced demand pattern (customers move from left to right). That is:\n",
    "\n",
    "\n",
    "- K = {1,2} --> people go from 1 to 6 and from 0 to 7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario(demand_input = {(1,6):2, (0,7):2, 'default':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy agent\n",
    "agent = CascadedQLearning(env=env)\n",
    "num_nodes = len(agent.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 100 | Reward: 70.27 | Revenue: 0.00 | ServedDemand: 33.00         | Reb. Cost: 196.20: 100%|██████████| 100/100 [03:39<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test Episodes\n",
    "test_episodes = 100\n",
    "epochs = trange(test_episodes) # build tqdm iterator for loop visualization\n",
    "max_steps = 100 # maximum length of episode\n",
    "np.random.seed(10)\n",
    "\n",
    "# book-keeping variables\n",
    "test_rewards = []\n",
    "test_revenue = []\n",
    "test_served_demand = []\n",
    "test_rebalancing_cost = []\n",
    "\n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            # Execure Random Policy\n",
    "            action_rl = [] # action for all nodes\n",
    "            for i in range(num_nodes):\n",
    "                action_rl.append(np.random.randint(low=0, high=4))\n",
    "            # get actual vehicle distributions vi (i.e. (x1*x2*..*xn)*num_vehicles)\n",
    "            v_d = agent.get_desired_distribution(action_rl)\n",
    "\n",
    "            # 1.3 Solve ILP - Minimal Distance Problem \n",
    "            t = env.time\n",
    "            accTuple = [(n,env.acc[n][t]) for n in env.acc]\n",
    "            accRLTuple = [(n, int(v_d_n)) for n, v_d_n in enumerate(v_d)]\n",
    "            edgeAttr = [(i,j,env.G.edges[i,j]['time']) for i,j in env.G.edges]\n",
    "            modPath = os.getcwd().replace('\\\\','/')+'/mod/'\n",
    "            OPTPath = os.getcwd().replace('\\\\','/')+'/OPT/Random/S2/'\n",
    "            if not os.path.exists(OPTPath):\n",
    "                os.makedirs(OPTPath)\n",
    "            datafile = OPTPath + f'data_{t}.dat'\n",
    "            resfile = OPTPath + f'res_{t}.dat'\n",
    "            with open(datafile,'w') as file:\n",
    "                file.write('path=\"'+resfile+'\";\\r\\n')\n",
    "                file.write('edgeAttr='+mat2str(edgeAttr)+';\\r\\n')\n",
    "                file.write('accInitTuple='+mat2str(accTuple)+';\\r\\n')\n",
    "                file.write('accRLTuple='+mat2str(accRLTuple)+';\\r\\n')\n",
    "            modfile = modPath+'minRebDistRebOnly.mod'\n",
    "            CPLEXPATH = \"C:/Program Files/ibm/ILOG/CPLEX_Studio1210/opl/bin/x64_win64/\"\n",
    "            my_env = os.environ.copy()\n",
    "            my_env[\"LD_LIBRARY_PATH\"] = CPLEXPATH\n",
    "            out_file =  OPTPath + f'out_{t}.dat'\n",
    "            with open(out_file,'w') as output_f:\n",
    "                subprocess.check_call([CPLEXPATH+\"oplrun\", modfile, datafile], stdout=output_f, env=my_env)\n",
    "            output_f.close()\n",
    "\n",
    "            # 3. collect results from file\n",
    "            flow = defaultdict(float)\n",
    "            with open(resfile,'r', encoding=\"utf8\") as file:\n",
    "                for row in file:\n",
    "                    item = row.strip().strip(';').split('=')\n",
    "                    if item[0] == 'flow':\n",
    "                        values = item[1].strip(')]').strip('[(').split(')(')\n",
    "                        for v in values:\n",
    "                            if len(v) == 0:\n",
    "                                continue\n",
    "                            i,j,f = v.split(',')\n",
    "                            flow[int(i),int(j)] = float(f)\n",
    "            action = [flow[i,j] for i,j in env.edges]\n",
    "\n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward # update sum of rewards over episode\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            obs = new_obs\n",
    "\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_rebalancing_cost:.2f}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_revenue.append(episode_revenue)\n",
    "        test_served_demand.append(episode_served_demand)\n",
    "        test_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Performance: \n",
      "\n",
      "Avg Reward: 52.44\n",
      "Total Revenue: 0.00\n",
      "Total Served Demand: 30.66\n",
      "Total Rebalancing Cost: 204.47\n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(test_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(test_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(test_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(test_rebalancing_cost):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
