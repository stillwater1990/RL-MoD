{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "\n",
    "from env import Scenario, AMoD, CascadedQLearning\n",
    "from util import mat2str, dictsum, moving_average\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "CPLEXPATH = \"C:/Program Files/ibm/ILOG/CPLEX_Studio1210/opl/bin/x64_win64/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1 <br>\n",
    "\n",
    "Given a 2x4 grid:\n",
    "\n",
    "| 0 | 2 | 4 | 6 |<br>\n",
    "| 1 | 3 | 5 | 7 | <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume the demand is generated according to K=2 periodic patterns. That is:\n",
    "\n",
    "\n",
    "- K = 1 --> people go from 0 to 7 and from 6 to 1;\n",
    "- K = 2 --> people go from 7 to 0 and from 1 to 6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy agent\n",
    "agent = CascadedQLearning(env=env)\n",
    "num_nodes = len(agent.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 100 | Reward: 4777.29 | Revenue: 5206.29 | ServedDemand: 715.00         | Reb. Cost: 429.00: 100%|██████████| 100/100 [03:08<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test Episodes\n",
    "test_episodes = 100\n",
    "epochs = trange(test_episodes) # build tqdm iterator for loop visualization\n",
    "max_steps = 100 # maximum length of episode\n",
    "np.random.seed(10)\n",
    "\n",
    "# book-keeping variables\n",
    "test_rewards = []\n",
    "test_revenue = []\n",
    "test_served_demand = []\n",
    "test_rebalancing_cost = []\n",
    "test_operating_cost = []\n",
    "\n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        episode_operating_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            # Execure MPC\n",
    "            action = env.MPC_exact(CPLEXPATH=CPLEXPATH)    \n",
    "\n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward # update sum of rewards over episode\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            episode_operating_cost += info['operating_cost']\n",
    "            obs = new_obs\n",
    "\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_operating_cost:.2f}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_revenue.append(episode_revenue)\n",
    "        test_served_demand.append(episode_served_demand)\n",
    "        test_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "        test_operating_cost.append(episode_operating_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Performance: \n",
      "\n",
      "Avg Reward: 5032.50\n",
      "Total Revenue: 5472.30\n",
      "Total Served Demand: 730.22\n",
      "Total Operating Cost: 439.80\n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(test_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(test_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(test_served_demand):.2f}')\n",
    "print(f'Total Operating Cost: {np.mean(test_operating_cost):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2x4 grid:\n",
    "\n",
    "| 0 | 2 | 4 | 6 |<br>\n",
    "| 1 | 3 | 5 | 7 | <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume the demand is generated according to an unbalanced demand pattern (customers move from left to right). That is:\n",
    "\n",
    "\n",
    "- K = {1,2} --> people go from 1 to 6 and from 0 to 7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario(demand_input = {(1,6):2, (0,7):2, 'default':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy agent\n",
    "agent = CascadedQLearning(env=env)\n",
    "num_nodes = len(agent.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 100 | Reward: 3213.98 | Revenue: 3646.58 | ServedDemand: 538.00         | Reb. Cost: 432.60: 100%|██████████| 100/100 [03:09<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Test Episodes\n",
    "test_episodes = 100\n",
    "epochs = trange(test_episodes) # build tqdm iterator for loop visualization\n",
    "max_steps = 100 # maximum length of episode\n",
    "np.random.seed(10)\n",
    "\n",
    "# book-keeping variables\n",
    "test_rewards = []\n",
    "test_revenue = []\n",
    "test_served_demand = []\n",
    "test_rebalancing_cost = []\n",
    "test_operating_cost = []\n",
    "\n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        episode_operating_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            # Execure MPC\n",
    "            action = env.MPC_exact(CPLEXPATH=CPLEXPATH)    \n",
    "\n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward # update sum of rewards over episode\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            episode_operating_cost += info['operating_cost']\n",
    "            obs = new_obs\n",
    "\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_operating_cost:.2f}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_revenue.append(episode_revenue)\n",
    "        test_served_demand.append(episode_served_demand)\n",
    "        test_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "        test_operating_cost.append(episode_operating_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Performance: \n",
      "\n",
      "Avg Reward: 3032.34\n",
      "Total Revenue: 3459.56\n",
      "Total Served Demand: 525.39\n",
      "Total Rebalancing Cost: 427.31\n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(test_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(test_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(test_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(test_operating_cost):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
