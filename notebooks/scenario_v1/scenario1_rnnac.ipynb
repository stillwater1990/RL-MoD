{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(sys.path[0].replace('notebooks/scenario_v1', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, namedtuple\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from src.misc.utils import mat2str, dictsum, moving_average\n",
    "from src.algos.ac.rnn_ac import Policy\n",
    "from src.envs.env_two_step import AMoD, Scenario\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Dirichlet\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "CPLEXPATH = \"/opt/ibm/ILOG/CPLEX_Studio128/opl/bin/x86-64_linux/\"\n",
    "src_path = sys.path[-1]\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/array/daga_data/Github/RL-MoD/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    x_ext = state[0]\n",
    "    x_temp = state[1]\n",
    "    concentrations, state_value = model(x_ext, x_temp)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Dirichlet(concentrations)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "    # the action to take (distribution of idle vehicles over cells)\n",
    "    return list(action.numpy())\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(),5.)\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize args\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "args = namedtuple('args', ('render', 'gamma', 'log_interval'))\n",
    "args.render= True\n",
    "args.gamma = 0.97\n",
    "args.log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scenario\n",
    "scenario = Scenario(sd=10,demand_input = {(1,6):2, (0,7):2, 'default':0.1}, fix_price=True)\n",
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model + optimizer\n",
    "model = Policy(env)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/opt/ibm/ILOG/CPLEX_Studio128/opl/bin/x86-64_linux/oplrun', '/mnt/array/daga_data/Github/RL-MoD/notebooks/scenario_v1/mod/matching.mod', '/mnt/array/daga_data/Github/RL-MoD/notebooks/scenario_v1/matching//mnt/array/daga_data/Github/RL-MoD/AC/v2.3/data_0.dat']' returned non-zero exit status 10.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b19bd97d6202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# take matching step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaxreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpax_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPLEXPATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCPLEXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"AC/v2.3/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpaxreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Select and perform an RL action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/array/daga_data/Github/RL-MoD/src/envs/env_two_step.py\u001b[0m in \u001b[0;36mpax_step\u001b[0;34m(self, paxAction, CPLEXPATH, PATH, platform)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rebalancing_cost'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpaxAction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# default matching algorithm used if isMatching is True, matching method will need the information of self.acc[t+1], therefore this part cannot be put forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mpaxAction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPLEXPATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCPLEXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplatform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaxAction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaxAction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# serving passengers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/array/daga_data/Github/RL-MoD/src/envs/env_two_step.py\u001b[0m in \u001b[0;36mmatching\u001b[0;34m(self, CPLEXPATH, PATH, platform)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmatchingPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'out_{}.dat'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCPLEXPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"oplrun\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0moutput_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyro1/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/opt/ibm/ILOG/CPLEX_Studio128/opl/bin/x86-64_linux/oplrun', '/mnt/array/daga_data/Github/RL-MoD/notebooks/scenario_v1/mod/matching.mod', '/mnt/array/daga_data/Github/RL-MoD/notebooks/scenario_v1/matching//mnt/array/daga_data/Github/RL-MoD/AC/v2.3/data_0.dat']' returned non-zero exit status 10."
     ]
    }
   ],
   "source": [
    "# book-keeping variables\n",
    "training_rewards = []\n",
    "training_revenue = []\n",
    "training_served_demand = []\n",
    "training_rebalancing_cost = []\n",
    "training_operating_cost = []\n",
    "\n",
    "# last_t_update = 0\n",
    "train_episodes = 50000 # num_of_episodes_with_same_epsilon x num_of_q_tables x num_epsilons          \n",
    "max_steps = 100 # maximum length of episode\n",
    "epochs = trange(train_episodes) # build tqdm iterator for loop visualization\n",
    "\n",
    "for i_episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        episode_operating_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            # take matching step \n",
    "            obs, paxreward, done, info = env.pax_step(CPLEXPATH=CPLEXPATH, PATH=src_path+\"AC/v2.3/\")\n",
    "            episode_reward += paxreward\n",
    "            # Select and perform an RL action\n",
    "            x_ext = torch.tensor([obs[0][n][env.time] for n in env.region] + [env.dacc[n][env.time] for n in env.region] + \\\n",
    "                    [env.price[i,j][t] for t in range(env.time, env.time+1) for i,j in env.demand] + [env.time]).float()\n",
    "            x_temp1 = torch.tensor([env.demand[i,j][t] for t in range(env.time, env.time+10) for i,j in env.demand]).view(1, 10, 56).float()\n",
    "            x_temp2 = torch.tensor([env.dacc[n][t] for n in env.region for t in range(env.time, env.time+10)]).view(1, 10, 8).float()\n",
    "            x_temp = torch.cat((x_temp1, x_temp2), dim=2)\n",
    "            state = (x_ext, x_temp)\n",
    "            action_rl = select_action(state)\n",
    "\n",
    "            # 1.2 get actual vehicle distributions vi (i.e. (x1*x2*..*xn)*num_vehicles)\n",
    "            v_d = model.get_desired_distribution(torch.tensor(action_rl))\n",
    "\n",
    "            # 1.3 Solve ILP - Minimal Distance Problem \n",
    "            # 1.3.1 collect inputs and build .dat file\n",
    "            t = env.time\n",
    "            accTuple = [(n,int(env.acc[n][t])) for n in env.acc]\n",
    "            accRLTuple = [(n, int(v_d_n)) for n, v_d_n in enumerate(v_d)]\n",
    "            edgeAttr = [(i,j,env.G.edges[i,j]['time']) for i,j in env.G.edges]\n",
    "            modPath = src_path.replace('\\\\','/')+'/mod/'\n",
    "            OPTPath = src_path.replace('\\\\','/')+'/OPT/AC/v2.3/'\n",
    "            if not os.path.exists(OPTPath):\n",
    "                os.makedirs(OPTPath)\n",
    "            datafile = OPTPath + f'data_{t}.dat'\n",
    "            resfile = OPTPath + f'res_{t}.dat'\n",
    "            with open(datafile,'w') as file:\n",
    "                file.write('path=\"'+resfile+'\";\\r\\n')\n",
    "                file.write('edgeAttr='+mat2str(edgeAttr)+';\\r\\n')\n",
    "                file.write('accInitTuple='+mat2str(accTuple)+';\\r\\n')\n",
    "                file.write('accRLTuple='+mat2str(accRLTuple)+';\\r\\n')\n",
    "\n",
    "            # 2. execute .mod file and write result on file\n",
    "            modfile = modPath+'minRebDistRebOnly.mod'\n",
    "            if CPLEXPATH is None:\n",
    "                CPLEXPATH = \"/opt/ibm/ILOG/CPLEX_Studio128/opl/bin/x86-64_linux/\"\n",
    "            my_env = os.environ.copy()\n",
    "            my_env[\"LD_LIBRARY_PATH\"] = CPLEXPATH\n",
    "            out_file =  OPTPath + f'out_{t}.dat'\n",
    "            with open(out_file,'w') as output_f:\n",
    "                subprocess.check_call([CPLEXPATH+\"oplrun\", modfile, datafile], stdout=output_f, env=my_env)\n",
    "            output_f.close()\n",
    "\n",
    "            # 3. collect results from file\n",
    "            flow = defaultdict(float)\n",
    "            with open(resfile,'r', encoding=\"utf8\") as file:\n",
    "                for row in file:\n",
    "                    item = row.strip().strip(';').split('=')\n",
    "                    if item[0] == 'flow':\n",
    "                        values = item[1].strip(')]').strip('[(').split(')(')\n",
    "                        for v in values:\n",
    "                            if len(v) == 0:\n",
    "                                continue\n",
    "                            i,j,f = v.split(',')\n",
    "                            flow[int(i),int(j)] = float(f)\n",
    "            rebAction = [flow[i,j] for i,j in env.edges]\n",
    "\n",
    "            # Take step\n",
    "            new_obs, rebreward, done, info = env.reb_step(rebAction)\n",
    "            episode_reward += rebreward\n",
    "            x_ext = torch.tensor([new_obs[0][n][env.time] for n in env.region] + [env.dacc[n][env.time] for n in env.region] + \\\n",
    "                    [env.price[i,j][t] for t in range(env.time, env.time+1) for i,j in env.demand] + [env.time]).float()\n",
    "            x_temp1 = torch.tensor([env.demand[i,j][t] for t in range(env.time, env.time+10) for i,j in env.demand]).view(1, 10, 56).float()\n",
    "            x_temp2 = torch.tensor([env.dacc[n][t] for n in env.region for t in range(env.time, env.time+10)]).view(1, 10, 8).float()\n",
    "            x_temp = torch.cat((x_temp1, x_temp2), dim=2)\n",
    "            new_state = (x_ext, x_temp)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            model.rewards.append(paxreward + rebreward)\n",
    "\n",
    "            # Move to the next state\n",
    "            # track performance over episode\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            episode_operating_cost += info['operating_cost']\n",
    "            obs, state = deepcopy(new_obs), deepcopy(new_state)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "        \n",
    "        \n",
    "        epochs.set_description(f\"Episode {i_episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "| Reb. Cost: {episode_rebalancing_cost:.2f} | Oper. Cost: {episode_operating_cost:.2f}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        training_rewards.append(episode_reward)\n",
    "        training_revenue.append(episode_revenue)\n",
    "        training_served_demand.append(episode_served_demand)\n",
    "        training_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "        training_operating_cost.append(episode_operating_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
