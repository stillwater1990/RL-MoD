{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "\n",
    "from env import Scenario, AMoD, CascadedQLearning\n",
    "from util import mat2str, dictsum, moving_average\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "CPLEXPATH = \"C:/Program Files/ibm/ILOG/CPLEX_Studio1210/opl/bin/x64_win64/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1 <br>\n",
    "\n",
    "Given a 2x4 grid:\n",
    "\n",
    "| 0 | 2 | 4 | 6 |<br>\n",
    "| 1 | 3 | 5 | 7 | <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume the demand is generated according to K=2 periodic patterns. That is:\n",
    "\n",
    "\n",
    "- K = 1 --> people go from 0 to 7 and from 6 to 1;\n",
    "- K = 2 --> people go from 7 to 0 and from 1 to 6;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL algorithm outputs the desired distribution of vehicles in each region, which is then fed as input for the following ILP: (Notice added constraint compared to Frazzoli paper) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha_{ij}} \\quad & \\sum_{i,j} T_{ij} \\alpha_{ij}\\\\\n",
    "\\textrm{s.t.} \\quad & \\sum_{j \\neq i} \\alpha_{ji} - \\alpha_{ij} \\geq v_i^d - v_i, \\quad \\forall i \\in \\mathcal{N}\\\\\n",
    "& \\sum_{j \\neq i} \\alpha_{ij} \\leq v_i, \\quad \\forall i \\in \\mathcal{N}\\\\\n",
    "  &\\alpha_{ij}\\geq0, \\quad \\forall i \\in \\mathcal{N}\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agent\n",
    "agent = CascadedQLearning(env=env)\n",
    "num_nodes = len(agent.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal parameters for agent\n",
    "params = {\"training_round_len\" : 600, # number of steps for which to train the same the same node (here 10 episodes of length 60) \n",
    "          \"epsilon\" : 1, # epsilon greedy initial parameter\n",
    "          \"k\" : 0, # counter for Q table indexing (top-down learning)\n",
    "          \"default_action\" : agent.action_space.index((0.5, 0.5))} # default behavior is to distribute vehicles equally\n",
    "\n",
    "# learning parameters\n",
    "alpha = 0.05 #learning rate                 \n",
    "discount_factor = 0.9 # discount factor in Bellman Equation\n",
    "# epsilon schedule parameters       \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "its = 15\n",
    "decays = [0.005]*5 + [0.01]*its + [0.03]*its + [0.05]*its\n",
    "\n",
    "train_episodes = 10*7*len(decays) # num_of_episodes_with_same_epsilon x num_of_q_tables x num_epsilons          \n",
    "max_steps = 100 # maximum length of episode\n",
    "epochs = trange(train_episodes) # build tqdm iterator for loop visualization\n",
    "\n",
    "# book-keeping variables\n",
    "training_rewards = []\n",
    "training_revenue = []\n",
    "training_served_demand = []\n",
    "training_rebalancing_cost = []\n",
    "\n",
    "idx = (params[\"k\"]//params[\"training_round_len\"])%num_nodes # Q table index (initially, top-most node)\n",
    "eps_idx = (params[\"k\"]//(scenario.tf*num_nodes*10))%len(decays) # epsilon index (low decay to high decay schedules)\n",
    "node = agent.nodes[idx] # select top-most node (i.e. (0, 1))\n",
    "decay = decays[eps_idx] # select initial decay rate\n",
    "switching_iters = np.linspace(0, (num_nodes-1)*params[\"training_round_len\"], num_nodes) \n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        state = agent.encode_state(agent.decode_state(obs[0], obs[1])[idx])\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            idx = (params[\"k\"]//params[\"training_round_len\"])%num_nodes # Q table index (initially, top-most node)\n",
    "            eps_idx = (params[\"k\"]//(scenario.tf*num_nodes*10))%len(decays) # epsilon index (low decay to high decay schedules)\n",
    "            node = agent.nodes[idx] # select node (i.e. (0, 1))\n",
    "            decay = decays[eps_idx] # select decay rate\n",
    "            \n",
    "            action, action_rl = agent.policy(obs, params, train=True, CPLEXPATH=CPLEXPATH, res_path=\"S1/\")\n",
    "    \n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action, isMatching=False)\n",
    "            new_state = agent.encode_state(agent.decode_state(new_obs[0], new_obs[1])[idx])\n",
    "\n",
    "            # Update Q table - Bellman Equation\n",
    "            action_q = action_rl[idx] # select action from Q table currently training\n",
    "            agent.Q[node][state, action_q] = agent.Q[node][state, action_q] + alpha*(reward+discount_factor*np.max(agent.Q[node][new_state, :])-agent.Q[node][state, action_q])\n",
    "\n",
    "            # track performance over episode\n",
    "            episode_reward += reward\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            obs, state = deepcopy(new_obs), deepcopy(new_state)\n",
    "\n",
    "            # update parameters\n",
    "            params[\"epsilon\"] = min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay*step)\n",
    "            params[\"k\"] += 1\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "            # checkpointing\n",
    "#             if (k)%59==0:\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/q_v1\", agent.Q)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/reward_v1\", training_rewards)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/revenue_v1\", training_revenue)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/served_demand_v1\", training_served_demand)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/cost_v1\", training_rebalancing_cost)\n",
    "            \n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_rebalancing_cost:.2f} | Decay: {decay}, Idx: {idx}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        training_rewards.append(episode_reward)\n",
    "        training_revenue.append(episode_revenue)\n",
    "        training_served_demand.append(episode_served_demand)\n",
    "        training_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Episodes\n",
    "test_episodes = 100\n",
    "epochs = trange(test_episodes) # build tqdm iterator for loop visualization\n",
    "np.random.seed(10)\n",
    "\n",
    "# book-keeping variables\n",
    "test_rewards = []\n",
    "test_revenue = []\n",
    "test_served_demand = []\n",
    "test_rebalancing_cost = []\n",
    "test_operating_cost = []\n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        episode_operating_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            action, action_rl = agent.policy(obs, params=dict(), train=False, CPLEXPATH=CPLEXPATH, res_path=\"S1/Test/\")\n",
    "    \n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action, isMatching=False)\n",
    "\n",
    "            # track performance over episode\n",
    "            episode_reward += reward\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            episode_operating_cost += info['operating_cost']\n",
    "            obs = deepcopy(new_obs)\n",
    "\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "| Oper. Cost: {episode_operating_cost:.2f} | Decay: {decay}, Idx: {idx}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_revenue.append(episode_revenue)\n",
    "        test_served_demand.append(episode_served_demand)\n",
    "        test_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig = plt.figure(figsize=(12,32))\n",
    "fig.add_subplot(411)\n",
    "plt.plot(training_rewards, label=\"Reward\")\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"J\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(412)\n",
    "plt.plot(training_revenue, label=\"Revenue\")\n",
    "plt.title(\"Episode Revenue\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(413)\n",
    "plt.plot(training_served_demand, label=\"Served Demand\")\n",
    "plt.title(\"Episode Served Demand\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Served Demand\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(414)\n",
    "plt.plot(training_rebalancing_cost, label=\"Reb. Cost\")\n",
    "plt.title(\"Episode Reb. Cost\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(training_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(training_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(training_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(training_rebalancing_cost):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results with moving average smoothing \n",
    "fig = plt.figure(figsize=(12,32))\n",
    "fig.add_subplot(411)\n",
    "plt.plot(moving_average(training_rewards, n=10), label=\"Avg. Reward\")\n",
    "plt.title(\"Avg. Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"J\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(412)\n",
    "plt.plot(moving_average(training_revenue, n=10), label=\"Avg. Revenue\")\n",
    "plt.title(\"Avg. Revenue\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(413)\n",
    "plt.plot(moving_average(training_served_demand, n=10), label=\"Avg. Served Demand\")\n",
    "plt.title(\"Avg. Served Demand\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Served Demand\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(414)\n",
    "plt.plot(moving_average(training_rebalancing_cost, n=10), label=\"Avg. Reb. Cost\")\n",
    "plt.title(\"Avg. Reb. Cost\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(test_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(test_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(test_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(test_rebalancing_cost):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2x4 grid:\n",
    "\n",
    "| 0 | 2 | 4 | 6 |<br>\n",
    "| 1 | 3 | 5 | 7 | <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "We assume the demand is generated according to an unbalanced demand pattern (customers move from left to right). That is:\n",
    "\n",
    "\n",
    "- K = {1,2} --> people go from 1 to 6 and from 0 to 7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = Scenario(demand_input = {(1,6):2, (0,7):2, 'default':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AMoD(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agent\n",
    "agent = CascadedQLearning(env=env)\n",
    "num_nodes = len(agent.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal parameters for agent\n",
    "params = {\"training_round_len\" : 600, # number of steps for which to train the same the same node (here 10 episodes of length 60) \n",
    "          \"epsilon\" : 1, # epsilon greedy initial parameter\n",
    "          \"k\" : 0, # counter for Q table indexing (top-down learning)\n",
    "          \"default_action\" : agent.action_space.index((0.5, 0.5))} # default behavior is to distribute vehicles equally\n",
    "\n",
    "# learning parameters\n",
    "alpha = 0.05 #learning rate                 \n",
    "discount_factor = 0.9 # discount factor in Bellman Equation\n",
    "# epsilon schedule parameters       \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "its = 15\n",
    "decays = [0.005]*5 + [0.01]*its + [0.03]*its + [0.05]*its\n",
    "\n",
    "train_episodes = 10*7*len(decays) # num_of_episodes_with_same_epsilon x num_of_q_tables x num_epsilons          \n",
    "max_steps = 100 # maximum length of episode\n",
    "epochs = trange(train_episodes) # build tqdm iterator for loop visualization\n",
    "\n",
    "# book-keeping variables\n",
    "training_rewards = []\n",
    "training_revenue = []\n",
    "training_served_demand = []\n",
    "training_rebalancing_cost = []\n",
    "\n",
    "idx = (params[\"k\"]//params[\"training_round_len\"])%num_nodes # Q table index (initially, top-most node)\n",
    "eps_idx = (params[\"k\"]//(scenario.tf*num_nodes*10))%len(decays) # epsilon index (low decay to high decay schedules)\n",
    "node = agent.nodes[idx] # select top-most node (i.e. (0, 1))\n",
    "decay = decays[eps_idx] # select initial decay rate\n",
    "switching_iters = np.linspace(0, (num_nodes-1)*params[\"training_round_len\"], num_nodes) \n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        state = agent.encode_state(agent.decode_state(obs[0], obs[1])[idx])\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            idx = (params[\"k\"]//params[\"training_round_len\"])%num_nodes # Q table index (initially, top-most node)\n",
    "            eps_idx = (params[\"k\"]//(scenario.tf*num_nodes*10))%len(decays) # epsilon index (low decay to high decay schedules)\n",
    "            node = agent.nodes[idx] # select node (i.e. (0, 1))\n",
    "            decay = decays[eps_idx] # select decay rate\n",
    "            \n",
    "            action, action_rl = agent.policy(obs, params, train=True, CPLEXPATH=CPLEXPATH, res_path=\"S2/\")\n",
    "    \n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action, isMatching=False)\n",
    "            new_state = agent.encode_state(agent.decode_state(new_obs[0], new_obs[1])[idx])\n",
    "\n",
    "            # Update Q table - Bellman Equation\n",
    "            action_q = action_rl[idx] # select action from Q table currently training\n",
    "            agent.Q[node][state, action_q] = agent.Q[node][state, action_q] + alpha*(reward+discount_factor*np.max(agent.Q[node][new_state, :])-agent.Q[node][state, action_q])\n",
    "\n",
    "            # track performance over episode\n",
    "            episode_reward += reward\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            obs, state = deepcopy(new_obs), deepcopy(new_state)\n",
    "\n",
    "            # update parameters\n",
    "            params[\"epsilon\"] = min_epsilon+(max_epsilon-min_epsilon)*np.exp(-decay*step)\n",
    "            params[\"k\"] += 1\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "            # checkpointing\n",
    "#             if (k)%59==0:\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/q_v1\", agent.Q)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/reward_v1\", training_rewards)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/revenue_v1\", training_revenue)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/served_demand_v1\", training_served_demand)\n",
    "#                 np.save(\"./saved_files/agents/checkpoint/cost_v1\", training_rebalancing_cost)\n",
    "            \n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_rebalancing_cost:.2f} | Decay: {decay}, Idx: {idx}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        training_rewards.append(episode_reward)\n",
    "        training_revenue.append(episode_revenue)\n",
    "        training_served_demand.append(episode_served_demand)\n",
    "        training_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Episodes\n",
    "test_episodes = 100\n",
    "np.random.seed(10)\n",
    "\n",
    "# book-keeping variables\n",
    "test_rewards = []\n",
    "test_revenue = []\n",
    "test_served_demand = []\n",
    "test_rebalancing_cost = []\n",
    "test_operating_cost = []\n",
    "\n",
    "for episode in epochs:\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_revenue = 0\n",
    "        episode_served_demand = 0\n",
    "        episode_rebalancing_cost = 0\n",
    "        episode_operating_cost = 0\n",
    "        for step in range(max_steps):\n",
    "            action, action_rl = agent.policy(obs, params=dict(), train=False, CPLEXPATH=CPLEXPATH, res_path=\"S2/Test/\")\n",
    "    \n",
    "            # Take step\n",
    "            new_obs, reward, done, info = env.step(action, isMatching=False)\n",
    "\n",
    "            # track performance over episode\n",
    "            episode_reward += reward\n",
    "            episode_revenue += info['revenue']\n",
    "            episode_served_demand += info['served_demand']\n",
    "            episode_rebalancing_cost += info['rebalancing_cost']\n",
    "            episode_operating_cost += info['operating_cost']\n",
    "            obs = deepcopy(new_obs)\n",
    "\n",
    "            # end episode if conditions reached\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        epochs.set_description(f\"Episode {episode+1} | Reward: {episode_reward:.2f} | Revenue: {episode_revenue:.2f} | ServedDemand: {episode_served_demand:.2f} \\\n",
    "        | Reb. Cost: {episode_rebalancing_cost:.2f} | Decay: {decay}, Idx: {idx}\")\n",
    "        #Adding the total reward and reduced epsilon values\n",
    "        test_rewards.append(episode_reward)\n",
    "        test_revenue.append(episode_revenue)\n",
    "        test_served_demand.append(episode_served_demand)\n",
    "        test_rebalancing_cost.append(episode_rebalancing_cost)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig = plt.figure(figsize=(12,32))\n",
    "fig.add_subplot(411)\n",
    "plt.plot(training_rewards, label=\"Reward\")\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"J\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(412)\n",
    "plt.plot(training_revenue, label=\"Revenue\")\n",
    "plt.title(\"Episode Revenue\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(413)\n",
    "plt.plot(training_served_demand, label=\"Served Demand\")\n",
    "plt.title(\"Episode Served Demand\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Served Demand\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(414)\n",
    "plt.plot(training_rebalancing_cost, label=\"Reb. Cost\")\n",
    "plt.title(\"Episode Reb. Cost\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(training_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(training_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(training_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(training_rebalancing_cost):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results with moving average smoothing \n",
    "fig = plt.figure(figsize=(12,32))\n",
    "fig.add_subplot(411)\n",
    "plt.plot(moving_average(training_rewards, n=10), label=\"Avg. Reward\")\n",
    "plt.title(\"Avg. Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"J\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(412)\n",
    "plt.plot(moving_average(training_revenue, n=10), label=\"Avg. Revenue\")\n",
    "plt.title(\"Avg. Revenue\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(413)\n",
    "plt.plot(moving_average(training_served_demand, n=10), label=\"Avg. Served Demand\")\n",
    "plt.title(\"Avg. Served Demand\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Served Demand\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(414)\n",
    "plt.plot(moving_average(training_rebalancing_cost, n=10), label=\"Avg. Reb. Cost\")\n",
    "plt.title(\"Avg. Reb. Cost\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "print(\"Average Performance: \\n\")\n",
    "print(f'Avg Reward: {np.mean(test_rewards):.2f}')\n",
    "print(f'Total Revenue: {np.mean(test_revenue):.2f}')\n",
    "print(f'Total Served Demand: {np.mean(test_served_demand):.2f}')\n",
    "print(f'Total Rebalancing Cost: {np.mean(test_rebalancing_cost):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
